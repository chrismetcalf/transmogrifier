#!/usr/bin/env ruby
#
# Uses dataset mappings to convert data into open standards
#

require 'thor'
require 'yaml'
require 'soda/client'
require 'time'
require 'active_support/core_ext/hash'
require 'httparty'
require 'erb'
require 'logger'
require 'octokit'
require 'base64'
require 'csv'

class Time
  require 'tzinfo'
  def in_tz(tzstring)
    tz = TZInfo::Timezone.get tzstring
    p = tz.period_for_utc self
    self + p.utc_offset
  end

  def to_socrata
    self.strftime("%Y-%m-%dT%H:%M:%S")
  end
end

class Box < Thor
  include Thor::Actions

  def initialize(*args)
    super

    # Shared clients
    @config = YAML.load(ERB.new(File.new(File.expand_path(options[:config])).read).result).deep_symbolize_keys
    @client = SODA::Client.new(@config[:odn])

    # Set up GitHub client
    Octokit.configure do |c|
      c.access_token = @config[:github][:access_token]
    end

    # Prefetch the whole tree
    @tree = Octokit.tree(@config[:github][:repo], "master", :recursive => true)[:tree]

    # We'll use this for memoization later
    @schemas = {}

    @log = Logger.new(STDOUT)
    @log.level = options[:verbose] ? Logger::DEBUG : Logger::INFO
  end

  option :config, :required => true, :type => :string
  desc 'bootstrap', 'Set our metadata datasets'
  def bootstrap
    @log.info "Creating metadata dataset"

    # Create a new NBE dataset
    dataset = create_dataset({
      :name => "Sync Status",
      :description => "Last sync and mapping information for Transmogrifier"
    })
    @log.info "Created dataset https://#{@config[:odn][:domain]}/d/#{dataset.id}"

    apply_schema(dataset.id, [ {
      :name => "ID",
      :fieldName => "id",
      :description => "Unique identifier",
      :dataTypeName => "text",
      :identifier => true
    }, {
      :name => "Source",
      :fieldName => "source",
      :description => "Source Dataset",
      :dataTypeName => "text"
    }, {
      :name => "Source Domain",
      :fieldName => "domain",
      :description => "Source Domain",
      :dataTypeName => "text"
    }, {
      :name => "Schema",
      :fieldName => "schema",
      :description => "Schema identifier",
      :dataTypeName => "text"
    }, {
      :name => "Last Updated",
      :fieldName => "last_updated",
      :description => "The latest update from this dataset",
      :dataTypeName => "number"
    } ])

    # Publish the dataset
    @client.post("/api/views/#{dataset.id}/publication.json")
    @log.info "Published dataset"

    @log.info "Add the following to the 'odn' key in your config:"
    @log.info "status_dataset: #{dataset.id}"
  end

  option :config, :required => true, :type => :string
  option :schema, :required => true
  desc 'setup_dataset', 'Set up a dataset for a given schema'
  def setup_dataset
    # Find that schema in our tree
    schema = fetch_yaml("schemas/#{options[:schema]}.yml")
    @log.info "Creating schema for \"#{schema[:name]}\""

    # Create a new NBE dataset
    dataset = create_dataset({
      :name => schema[:name],
      :description => schema[:benefits]
    })
    @log.info "Created dataset https://#{@config[:odn][:domain]}/d/#{dataset.id}"

    apply_schema(dataset.id, schema[:columns].collect { |col|
      {
        :name => col[:name],
        :fieldName => col[:field_name],
        :description => col[:description],
        :dataTypeName => col[:data_type]
      }
    } + [ {
      :name => "Global ID",
      :fieldName => "global_id",
      :description => "Unique identifier for each row",
      :dataTypeName => "text",
      :identifier => true
    }, {
      :name => "Source",
      :fieldName => "source",
      :description => "URI to the source of this data",
      :dataTypeName => "text"
    } ])

    # Publish the dataset
    @client.post("/api/views/#{dataset.id}/publication.json")
    @log.info "Published dataset"

    @log.info "Add the following to the 'datasets' key in your config:"
    @log.info "#{dataset.id}: #{options[:schema]}"
  end

  option :config, :required => true, :type => :string
  desc 'update', 'Merge and update datasets'
  def update
    setup_config_json @config[:odn]

    @config[:datasets].each do |uid, schema|
      puts "Updating #{uid} with #{schema}..."
      @schemas[schema] ||= fetch_yaml("schemas/#{schema}.yml")
      headers = (["source", "global_id"] + @schemas[schema][:columns].collect { |c| c[:field_name] }).sort

      # Create a temporary file where we'll collect our results
      tmpfile = Tempfile.new([schema.gsub("/", "-"), ".csv"])
      puts "Writing output for #{schema} to #{tmpfile.path}..."
      CSV.open(tmpfile, "wb") do |csv|
        # Write out a headers
        csv << headers

        # Iterate through all of the mappings for that schema
        Octokit.search_code("#{schema} path:/mappings repo:#{@config[:github][:repo]}")[:items].each do |f|
          begin
            puts "Processing #{f.path}..."
            m = fetch_yaml(f.path)

            # Fetch the whole dataset in chunks
            offset = 0
            loop do
              query = (m[:mapping][:query] || {}).merge({
                  :$select => [ 
                    (m[:mapping][:query][:$select] || "*"),
                    ":id AS global_id, '#{m[:mapping][:base]}' AS source"
                  ].join(", "),
                  :$limit => @config[:batch_size],
                  :$offset => offset
                })

              if m[:mapping][:query][:$group]
                query[:$group] = [
                  m[:mapping][:query][:$group],
                  "global_id, source"
                ].join(", ")
              end

              results = @client.get(
                m[:mapping][:base].gsub(/\.csv$/, ".json"),
                query
              )

              # Bail!
              break if results.size <= 0

              results.each do |res|
                csv << headers.collect do |h| 
                  if res[h].is_a?(Hashie::Mash) && res[h].type == "Point"
                    "(#{res[h].coordinates[1]},#{res[h].coordinates[0]})"
                  else
                    res[h]
                  end
                end
              end

              offset += @config[:batch_size]
            end
          rescue Exception => e
            puts "Error: #{e.inspect}, #{e.http_body}"
          end
        end
      end

      puts "Updating dataset with DataSync..."
      Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{tmpfile.path} -i #{uid} -t IntegrationJob -m replace -h true")
    end
  end

  option :config, :required => true, :type => :string
  desc 'supertrooper', 'Merge and aggregate crime datasets from moto.data.socrata.com'
  def supertrooper
    setup_config_json @config[:supertrooper]

    # Create a temporary file where we'll collect our results
    tmpfile = Tempfile.new(["supertroopers", ".csv"])
    puts "Writing output for supertroopers to #{tmpfile.path}..."
    CSV.open(tmpfile, "wb") do |csv|
      # Write out a headers
      csv << [:id, :name, :type, :date, :year, :month, :crime_type, :crime_count, :crime_rate, :location, :source_agency, :base_population]

      # Fetch the agencies dataset
      @client.get(@config[:supertrooper][:agencies], { :agency_type => "Police Dept" }).each do |agency|
        # Figure out what geo this is most likely near
        cities = @client.get(@config[:supertrooper][:gazeteer],
                              {
                                :type => :place,
                                :$order => "distance_in_meters(location, 'POINT (#{agency.center.coordinates.join(' ')})') ASC",
                                :$select => "*, distance_in_meters(location, 'POINT (#{agency.center.coordinates.join(' ')})') AS range",
                                :$q => agency.city,
                                :$limit => 5

        })

        if(cities.count > 0 && cities.first.range.to_i < 5000)
          # Fetch crime counts for this city
          @client.get("https://#{agency.domain}/resource/#{agency.incident_dataset}",
                                {
                                  :$group => 'date_trunc_ym(incident_datetime), parent_incident_type',
                                  :$select => 'date_trunc_ym(incident_datetime) AS year_month, parent_incident_type, COUNT(*) AS total',
                                  :$where => 'incident_datetime IS NOT NULL'
          }).each do |res|
            ym = DateTime.parse(res.year_month)

            # Dump to our CSV!
            csv << [
              cities.first.id,
              cities.first.name,
              "place",
              res.year_month,
              ym.year,
              ym.month,
              res.parent_incident_type,
              res.total.to_i,
              res.total.to_f / cities.first.population.to_f,
              "(#{cities.first.location.coordinates.reverse.join(",")})",
              agency.agency_name,
              cities.first.population.to_i
            ]
          end
        end
      end

      puts "Updating dataset with DataSync..."
      Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{tmpfile.path} -i #{@config[:supertrooper][:dest_uid]} -t IntegrationJob -m replace -h true")
    end
  end

  ########################################
  # Utility
  ########################################
  desc "console", "Console, damnit"
  option :config, :required => true
  def console
    require 'pry'; binding.pry
  end

  no_commands do
    # Fetch a file from github
    def fetch_yaml(path)
      YAML.load(Base64.decode64(Octokit.content(@config[:github][:repo],
        :path => @tree.select { |f| f.path == path }.first.path
      ).content)).deep_symbolize_keys
    end

    def create_dataset(meta)
      return @client.post("/api/views.json?nbe=true", meta)
    end

    def apply_schema(uid, schema)
      schema.each do |col|
        is_id = col.delete(:identifier)
        col = @client.post("/api/views/#{uid}/columns.json", col)

        if is_id
          @client.put("/api/views/#{uid}.json", { :rowIdentifierColumnId => col.id })
        end
      end
    end

    def setup_config_json(config)
      # Generate a config.json
      File.open('config.json', 'w') { |f| 
        f.write({
          :domain => config[:domain],
          :username => config[:username],
          :password => config[:password],
          :appToken => config[:app_token]
        }.to_json())
      }
    end
  end
end

Box.start
