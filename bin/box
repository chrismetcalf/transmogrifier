#!/usr/bin/env ruby
#
# Uses dataset mappings to convert data into open standards
#

require 'thor'
require 'yaml'
require 'soda/client'
require 'time'
require 'active_support/core_ext/hash'
require 'active_support/inflector'
require 'httparty'
require 'erb'
require 'logger'
require 'octokit'
require 'base64'
require 'csv'
require 'sqlite3'
require 'fileutils'
require 'ruby-progressbar'
require 'roo'
require 'roo-xls'
require 'enumerator'

class Time
  require 'tzinfo'
  def in_tz(tzstring)
    tz = TZInfo::Timezone.get tzstring
    p = tz.period_for_utc self
    self + p.utc_offset
  end

  def to_socrata
    self.strftime("%Y-%m-%dT%H:%M:%S")
  end
end

class Box < Thor
  include Thor::Actions

  def initialize(*args)
    super

    # Shared clients
    @config = YAML.load(ERB.new(File.new(File.expand_path(options[:config])).read).result).deep_symbolize_keys
    @client = SODA::Client.new(@config[:odn])

    # Set up GitHub client
    Octokit.configure do |c|
      c.access_token = @config[:github][:access_token]
    end

    # Prefetch the whole tree
    @tree = Octokit.tree(@config[:github][:repo], "master", :recursive => true)[:tree]

    # We'll use this for memoization later
    @schemas = {}

    @log = Logger.new(STDOUT)
    @log.level = options[:verbose] ? Logger::DEBUG : Logger::INFO
  end

  option :config, :required => true, :type => :string
  desc 'bootstrap', 'Set our metadata datasets'
  def bootstrap
    @log.info "Creating metadata dataset"

    # Create a new NBE dataset
    dataset = create_dataset({
      :name => "Sync Status",
      :description => "Last sync and mapping information for Transmogrifier"
    })
    @log.info "Created dataset https://#{@config[:odn][:domain]}/d/#{dataset.id}"

    apply_schema(dataset.id, [ {
      :name => "ID",
      :fieldName => "id",
      :description => "Unique identifier",
      :dataTypeName => "text",
      :identifier => true
    }, {
      :name => "Source",
      :fieldName => "source",
      :description => "Source Dataset",
      :dataTypeName => "text"
    }, {
      :name => "Source Domain",
      :fieldName => "domain",
      :description => "Source Domain",
      :dataTypeName => "text"
    }, {
      :name => "Schema",
      :fieldName => "schema",
      :description => "Schema identifier",
      :dataTypeName => "text"
    }, {
      :name => "Last Updated",
      :fieldName => "last_updated",
      :description => "The latest update from this dataset",
      :dataTypeName => "number"
    } ])

    # Publish the dataset
    @client.post("/api/views/#{dataset.id}/publication.json")
    @log.info "Published dataset"

    @log.info "Add the following to the 'odn' key in your config:"
    @log.info "status_dataset: #{dataset.id}"
  end

  option :config, :required => true, :type => :string
  option :schema, :required => true
  desc 'setup_dataset', 'Set up a dataset for a given schema'
  def setup_dataset
    # Find that schema in our tree
    schema = fetch_yaml("schemas/#{options[:schema]}.yml")
    @log.info "Creating schema for \"#{schema[:name]}\""

    # Create a new NBE dataset
    dataset = create_dataset({
      :name => schema[:name],
      :description => schema[:benefits]
    })
    @log.info "Created dataset https://#{@config[:odn][:domain]}/d/#{dataset.id}"

    apply_schema(dataset.id, schema[:columns].collect { |col|
      {
        :name => col[:name],
        :fieldName => col[:field_name],
        :description => col[:description],
        :dataTypeName => col[:data_type]
      }
    } + [ {
      :name => "Global ID",
      :fieldName => "global_id",
      :description => "Unique identifier for each row",
      :dataTypeName => "text",
      :identifier => true
    }, {
      :name => "Source",
      :fieldName => "source",
      :description => "URI to the source of this data",
      :dataTypeName => "text"
    } ])

    # Publish the dataset
    @client.post("/api/views/#{dataset.id}/publication.json")
    @log.info "Published dataset"

    @log.info "Add the following to the 'datasets' key in your config:"
    @log.info "#{dataset.id}: #{options[:schema]}"
  end

  option :config, :required => true, :type => :string
  desc 'update', 'Merge and update datasets'
  def update
    setup_config_json @config[:odn]

    @config[:datasets].each do |uid, schema|
      puts "Updating #{uid} with #{schema}..."
      @schemas[schema] ||= fetch_yaml("schemas/#{schema}.yml")
      headers = (["source", "global_id"] + @schemas[schema][:columns].collect { |c| c[:field_name] }).sort

      # Create a temporary file where we'll collect our results
      tmpfile = Tempfile.new([schema.gsub("/", "-"), ".csv"])
      puts "Writing output for #{schema} to #{tmpfile.path}..."
      CSV.open(tmpfile, "wb") do |csv|
        # Write out a headers
        csv << headers

        # Iterate through all of the mappings for that schema
        Octokit.search_code("#{schema} path:/mappings repo:#{@config[:github][:repo]}")[:items].each do |f|
          begin
            puts "Processing #{f.path}..."
            m = fetch_yaml(f.path)

            # Fetch the whole dataset in chunks
            offset = 0
            loop do
              query = (m[:mapping][:query] || {}).merge({
                  :$select => [
                    (m[:mapping][:query][:$select] || "*"),
                    ":id AS global_id, '#{m[:mapping][:base]}' AS source"
                  ].join(", "),
                  :$limit => @config[:batch_size],
                  :$offset => offset
                })

              if m[:mapping][:query][:$group]
                query[:$group] = [
                  m[:mapping][:query][:$group],
                  "global_id, source"
                ].join(", ")
              end

              results = @client.get(
                m[:mapping][:base].gsub(/\.csv$/, ".json"),
                query
              )

              # Bail!
              break if results.size <= 0

              results.each do |res|
                csv << headers.collect do |h|
                  if res[h].is_a?(Hashie::Mash) && res[h].type == "Point"
                    "(#{res[h].coordinates[1]},#{res[h].coordinates[0]})"
                  else
                    res[h]
                  end
                end
              end

              offset += @config[:batch_size]
            end
          rescue SODA::Exception => e
            puts "Error: #{e.inspect}, #{e.http_body}"
          end
        end
      end

      puts "Updating dataset with DataSync..."
      Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{tmpfile.path} -i #{uid} -t IntegrationJob -m replace -h true")
    end
  end

  # Pull FBC UCR Data
  option :config, :required => true, :type => :string
  desc 'macklin', 'Merge and aggregate crime datasets from FBI UCR Data'
  def macklin
    puts "\"Burt Macklin, FBI. The best damn agent they had, until I was framed for a crime I didn't commit... Stealing the President's rubies.\" ðŸ˜Ž"
    update = []

    # Pull down our place data to save time
    places = @client.get(@config[:macklin][:roster], {
      :$select => "id, name",
      # :$where => "type IN ('region.place', 'region.township')",
      :$where => "type IN ('region.place')",
      :$limit => 100000,
    }).inject({}) { |mem, place|
      mem[place.name] = place.id
      mem
    }

    # We need a mapping of states too
    states = @client.get(@config[:macklin][:states], {
      :$select => "stusps, name"
    })

    # Generate our stop words regex
    stop_words = Regexp.new("\s*(#{@config[:macklin][:stop_words].join("|")})\s*", Regexp::IGNORECASE)

    # Keep an audit roll
    audit = [[:ucr_city, :ucr_state, :ucr_year, :acs_place_id, :acs_place_name]]

    tmpfile = Tempfile.new(["macklin", ".csv"])
    puts "Writing output to #{tmpfile.path}..."
    CSV.open(tmpfile, "wb") do |csv|
      # Write out our header
      csv << [:key, :id, :name, :type, :variable, :value, :year, :crime_type, :place_name]

      Dir.mktmpdir("macklin") do |dir|
        @config[:macklin][:files].each do |year, url|
          filename = "#{dir}/#{year}.xls"

          puts "Fetching data for #{year} from #{url}..."
          `curl -sL #{url} > #{filename}`

          # Parse our XLS
          begin
            sheet = Roo::Spreadsheet.open(filename).sheet(0)
          rescue Exception => e
            $stderr.puts "Error reading #{filename}, probably it was empty..."
            $stderr.puts e.inspect
            require 'pry'; binding.pry
            next
          end

          columns = sheet.row(4)
          state = nil
          state_code = nil

          # We'll eliminate cities from this datastructure each time we see one
          places_audit = places.dup

          # Loop through our cities
          for num in 5...sheet.last_row
            row = sheet.row(num)

            # City is in column B
            city = row[1]
            next if city.nil?

            # Clean up our city name to improve matches
            city = city.gsub(/\d+/, '').gsub(stop_words, '').strip

            # State is column A
            if !row[0].nil?
              state = row[0].gsub(/\d+/, '').titlecase
              state_code = states.find { |s| s.name.downcase == state.downcase }.stusps
            end

            # Do a "fuzzy" lookup for the city in our places table
            place_name = places.keys.grep(/^#{city}.*#{state_code}$/i).first
            place_id = places[place_name]

            # Remove it from our audit hash
            places_audit.delete(place_id)

            if place_id.nil?
              $stderr.puts "No match found for: '#{city}, #{state_code}'"
              audit << [
                city,
                state,
                year,
                nil,
                nil
              ]
              next
            end

            all_crimes = 0
            population = nil
            for col in 2..row.size
              next if columns[col].nil?

              crime_type = columns[col]
              .gsub(/\s+/, ' ')
              .gsub(/\d+/, '')
              .gsub(/^.*Larceny.*$/, 'Larceny')

              if row[col]
                # Count
                csv << [
                  "#{place_id}--#{year}--#{crime_type}--count",  # key
                  place_id,                                      # id
                  "#{city}, #{state}",                           # name
                  :place,                                        # place
                  :count,                                        # variable
                  row[col],                                      # value
                  year,                                          # year
                  crime_type,                                    # crime_type
                  place_name                                     # place_name
                ]

                # Aggregate our "All Crimes" metric
                all_crimes += row[col].to_i

                # Only if we have a population do we add the rate
                if row[2] && row[2].to_f > 0.0
                  # Rate
                  csv << [
                    "#{place_id}--#{year}--#{crime_type}--rate",  # key
                    place_id,                                     # id
                    "#{city}, #{state}",                          # name
                    :place,                                       # place
                    :rate,                                        # variable
                    row[col].to_f / row[1].to_f,                  # value
                    year,                                         # year
                    crime_type,                                   # crime_type
                    place_name                                    # place_name
                  ]

                  population = row[2].to_f
                else
                  $stderr.puts "Skipping rate for for: #{city}, #{state}'"
                end
              end
            end # columns

            # All Crimes Count
            csv << [
              "#{place_id}--#{year}--All Crimes--count",  # key
              place_id,                                      # id
              "#{city}, #{state}",                           # name
              :place,                                        # place
              :count,                                        # variable
              all_crimes,                                    # value
              year,                                          # year
              "All Crimes",                                  # crime_type
              place_name                                     # place_name
            ]

            if population
              # Rate
              csv << [
                "#{place_id}--#{year}--All Crimes--rate",  # key
                place_id,                                     # id
                "#{city}, #{state}",                          # name
                :place,                                       # place
                :rate,                                        # variable
                all_crimes / population,                      # value
                year,                                         # year
                "All Crimes",                                 # crime_type
                place_name                                    # place_name
              ]
            end
          end # cities

          # Add places we've missed to the audit log
          places_audit.each do |id, name|
            audit << [
              nil,
              nil,
              year,
              id,
              name
            ]
          end
          require 'pry'; binding.pry
        end # years
      end # tmpdir

      if @config[:macklin][:dest_uid]
        setup_config_json @config[:macklin]
        puts "Updating dataset with DataSync..."
        Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{tmpfile.path} -i #{@config[:macklin][:dest_uid]} -t IntegrationJob -m replace -h true")
      else
        puts "Copying #{tmpfile.path} to #{Dir.pwd}..."
        FileUtils.cp tmpfile.path, Dir.pwd
      end
    end # csv

    puts "\"Macklin, you son of a bitch...\" ðŸ˜Ž"
  end

  # Magic to get Spatialite working:
  option :config, :required => true, :type => :string
  desc 'supertrooper', 'Merge and aggregate crime datasets from moto.data.socrata.com'
  def supertrooper
    setup_config_json @config[:supertrooper]

    # We'll use SQLite for some magic because I hate doing math
    db_file = 'superdb.sqlite'
    puts "Initializing superdb at #{db_file}..."
    db = SQLite3::Database.new db_file

    # Create the table we'll accumulate stats in
    db.execute "DROP TABLE IF EXISTS crime_counts;"
    db.execute <<-SQL
      CREATE TABLE crime_counts (
        geoid VARCHAR(30),
        year INT,
        month INT,
        parent_incident_type VARCHAR(100),
        count INT
      );
    SQL
    db.execute <<-SQL
      CREATE UNIQUE INDEX crime_counts_index
        ON crime_counts (geoid, year, month, parent_incident_type);
    SQL
    db.busy_timeout = 5000 # Give up to 500ms to clear a lock

    # Load the table we'll use for places lookup
    puts "Loading places DB from #{@config[:supertrooper][:places_db]}..."
    places_db = SQLite3::Database.new @config[:supertrooper][:places_db]

    # Let's enable spatialite so we can do geospatial queries
    places_db.enable_load_extension(1)
    places_db.load_extension "mod_spatialite"
    places_db.enable_load_extension(0)

    lookup_stmt = places_db.prepare <<-SQL
      SELECT
        geoid,
        name
      FROM places p
      WHERE
        Contains(
          the_geom,
          MakePoint(:longitude, :latitude)
        )
        AND p.rowid IN (
          SELECT pkid
          FROM idx_places_the_geom
          WHERE
            xmin <= :longitude
            AND xmax >= :longitude
            AND ymin <= :latitude
            AND ymax >= :latitude
        )
      LIMIT 1;
    SQL

    agencies = @client.get(@config[:supertrooper][:agencies], :$where => @config[:supertrooper][:agency_filter], :$order => "agency_name", :$limit => 50000)
    bar = ProgressBar.create(
      title: "Agencies", 
      format: '%a |%bðŸ‘®%i| %p%% %t',
      total: agencies.size)

    agencies.each do |agency|
      results = []
      offset = 0
      loop do
        results = @client.get("https://#{agency.domain}/resource/#{agency.incident_dataset}",
                              :$where => "incident_datetime >= '2015-01-01'",
                              :$order => "incident_datetime",
                              :$offset => offset,
                              :$limit => @config[:batch_size])
        break if results.size <= 0

        results.each do |res|
          # Look up what geom its in
          place = lookup_stmt.execute(:longitude => res.longitude, :latitude => res.latitude).next
          next if place.nil?

          geoid = place[0]
          date = DateTime.parse(res.incident_datetime)

          # Increment the appropriate counter
          db.execute <<-SQL
            INSERT OR REPLACE INTO crime_counts
            VALUES ('#{geoid}', #{date.year}, #{date.month}, '#{res.parent_incident_type}',
              COALESCE(
                (SELECT count FROM crime_counts
                  WHERE
                    geoid = '#{geoid}'
                    AND year = #{date.year}
                    AND month = #{date.month}
                    AND parent_incident_type = '#{res.parent_incident_type}'),
                0) + 1);
          SQL
          bar.refresh
        end

        offset += @config[:batch_size]
      end

      bar.increment
    end

    require 'pry'; binding.pry

    # Fetch place details and populations
    gazeteer = @client.get(@config[:supertrooper][:gazeteer], :type => "place", :$limit => 50000).inject({}) { |mem, c| mem[c.id] = c; mem }

    # Year-Month
    ym_tmpfile = Tempfile.new(["supertroopers-ym", ".csv"])
    puts "Writing year-month output for supertroopers to #{ym_tmpfile.path}..."
    CSV.open(ym_tmpfile, "wb") do |csv|
      # Write out a headers
      csv << [:id, :name, :type, :date, :year, :month, :crime_type, :crime_count, :crime_rate, :location, :base_population]

      db.execute("SELECT geoid, year, month, parent_incident_type, count FROM crime_counts").each do |res|

        # Fetch our matching place
        place = gazeteer["1600000US" + res[0]]

        if place.nil?
          puts "No match for #{res.inspect}"
          next
        elsif(place.population.nil? || place.population.empty? || place.population.to_i <= 0)
          puts "Zero or empty population for #{res.inspect}"
          next
        end

        # Dump to our CSV!
        csv << [
          place.id,
          place.name,
          "place",
          "#{res[1]}-#{res[2]}-01T00:00:00",
          res[1],
          res[2],
          res[3],
          res[4],
          res[4].to_f / place.population.to_f,
          "(#{place.location.coordinates.reverse.join(",")})",
          place.population.to_i
        ]
      end

      if(@config[:supertrooper][:ym_dest_uid])
        puts "Updating dataset with DataSync..."
        Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{ym_tmpfile.path} -i #{@config[:supertrooper][:ym_dest_uid]} -t IntegrationJob -m replace -h true")
      else
        puts "Copying #{ym_tmpfile.path} to #{Dir.pwd}..."
        FileUtils.cp ym_tmpfile.path, Dir.pwd
      end
    end

    # Yearly
    y_tmpfile = Tempfile.new(["supertroopers-y", ".csv"])
    puts "Writing year-month output for supertroopers to #{y_tmpfile.path}..."
    CSV.open(y_tmpfile, "wb") do |csv|
      # Write out a headers
      csv << [:id, :name, :type, :date, :year, :crime_type, :crime_count, :crime_rate, :location, :base_population]

      db.execute("SELECT geoid, year, parent_incident_type, SUM(count) FROM crime_counts GROUP BY geoid, year, parent_incident_type").each do |res|

        # Fetch our matching place
        place = gazeteer["1600000US" + res[0]]

        if place.nil?
          puts "No match for #{res.inspect}"
          next
        elsif(place.population.nil? || place.population.empty? || place.population.to_i <= 0)
          puts "Zero or empty population for #{res.inspect}"
          next
        end

        # Dump to our CSV!
        csv << [
          place.id,
          place.name,
          "place",
          "#{res[1]}-01-01T00:00:00",
          res[1],
          res[2],
          res[3],
          res[3].to_f / place.population.to_f,
          "(#{place.location.coordinates.reverse.join(",")})",
          place.population.to_i
        ]
      end

      if @config[:supertrooper][:y_dest_uid]
        puts "Updating dataset with DataSync..."
        Kernel.exec("java -jar ./DataSync-1.6.2.jar -sc control.json -ph true -c config.json -f #{y_tmpfile.path} -i #{@config[:supertrooper][:y_dest_uid]} -t IntegrationJob -m replace -h true")
      else
        puts "Copying #{y_tmpfile.path} to #{Dir.pwd}..."
        FileUtils.cp y_tmpfile.path, Dir.pwd
      end
    end

    puts "ðŸš¨ðŸš¨ðŸš¨ Done!!! ðŸš¨ðŸš¨ðŸš¨"
  end

  ########################################
  # Utility
  ########################################
  desc "console", "Console, damnit"
  option :config, :required => true
  def console
    require 'pry'; binding.pry
  end

  no_commands do
    # Fetch a file from github
    def fetch_yaml(path)
      YAML.load(Base64.decode64(Octokit.content(@config[:github][:repo],
        :path => @tree.select { |f| f.path == path }.first.path
      ).content)).deep_symbolize_keys
    end

    def create_dataset(meta)
      return @client.post("/api/views.json?nbe=true", meta)
    end

    def apply_schema(uid, schema)
      schema.each do |col|
        is_id = col.delete(:identifier)
        col = @client.post("/api/views/#{uid}/columns.json", col)

        if is_id
          @client.put("/api/views/#{uid}.json", { :rowIdentifierColumnId => col.id })
        end
      end
    end

    def setup_config_json(config)
      # Generate a config.json
      File.open('config.json', 'w') { |f|
        f.write({
          :domain => config[:domain],
          :username => config[:username],
          :password => config[:password],
          :appToken => config[:app_token]
        }.to_json())
      }
    end
  end
end

Box.start
